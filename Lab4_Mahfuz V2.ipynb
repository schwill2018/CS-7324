{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b2ab66-48c6-4b73-af44-a05587132ef7",
   "metadata": {
    "id": "b0b2ab66-48c6-4b73-af44-a05587132ef7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from mlxtend.evaluate import mcnemar\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0Q2WhyYlvFH",
   "metadata": {
    "id": "e0Q2WhyYlvFH"
   },
   "source": [
    "## 1. Load, Split, and Balance (1.5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10qi5kl0AC",
   "metadata": {
    "id": "4e10qi5kl0AC"
   },
   "source": [
    "[.5 points] (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. (2) Remove any observations that having missing data. (3) Encode any string data as integers for now. (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a7eb57-e742-429e-bf12-42b99807fea0",
   "metadata": {
    "id": "64a7eb57-e742-429e-bf12-42b99807fea0"
   },
   "outputs": [],
   "source": [
    "census = pd.read_csv('acs2017_census_tract_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "di5Na9qaPiz1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "di5Na9qaPiz1",
    "outputId": "29c3cc0f-6386-4d27-db1e-e387fdcbea00"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "census"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-79fe2a15-6c60-41e9-ab3c-397d32b4a0b1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TractId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001020100</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1845</td>\n",
       "      <td>899</td>\n",
       "      <td>946</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>881</td>\n",
       "      <td>74.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001020200</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>2172</td>\n",
       "      <td>1167</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>852</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001020300</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>3385</td>\n",
       "      <td>1533</td>\n",
       "      <td>1852</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>1482</td>\n",
       "      <td>73.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001020400</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>4267</td>\n",
       "      <td>2001</td>\n",
       "      <td>2266</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1849</td>\n",
       "      <td>75.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001020500</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>9965</td>\n",
       "      <td>5054</td>\n",
       "      <td>4911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4787</td>\n",
       "      <td>71.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1001020600</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>3620</td>\n",
       "      <td>1765</td>\n",
       "      <td>1855</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.7</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>1364</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001020700</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>3420</td>\n",
       "      <td>1459</td>\n",
       "      <td>1961</td>\n",
       "      <td>4.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>1418</td>\n",
       "      <td>77.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1001020801</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>2913</td>\n",
       "      <td>1495</td>\n",
       "      <td>1418</td>\n",
       "      <td>4.2</td>\n",
       "      <td>86.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>24.6</td>\n",
       "      <td>1369</td>\n",
       "      <td>72.7</td>\n",
       "      <td>17.5</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1001020802</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>11333</td>\n",
       "      <td>5488</td>\n",
       "      <td>5845</td>\n",
       "      <td>1.4</td>\n",
       "      <td>81.8</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>27.4</td>\n",
       "      <td>4857</td>\n",
       "      <td>70.1</td>\n",
       "      <td>22.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1001020900</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>6167</td>\n",
       "      <td>3111</td>\n",
       "      <td>3056</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>35.6</td>\n",
       "      <td>2781</td>\n",
       "      <td>80.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79fe2a15-6c60-41e9-ab3c-397d32b4a0b1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-79fe2a15-6c60-41e9-ab3c-397d32b4a0b1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-79fe2a15-6c60-41e9-ab3c-397d32b4a0b1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-c51c0d6b-7108-42c6-ad62-5b5420cecc87\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c51c0d6b-7108-42c6-ad62-5b5420cecc87')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-c51c0d6b-7108-42c6-ad62-5b5420cecc87 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      TractId    State          County  TotalPop   Men  Women  Hispanic  \\\n",
       "0  1001020100  Alabama  Autauga County      1845   899    946       2.4   \n",
       "1  1001020200  Alabama  Autauga County      2172  1167   1005       1.1   \n",
       "2  1001020300  Alabama  Autauga County      3385  1533   1852       8.0   \n",
       "3  1001020400  Alabama  Autauga County      4267  2001   2266       9.6   \n",
       "4  1001020500  Alabama  Autauga County      9965  5054   4911       0.9   \n",
       "5  1001020600  Alabama  Autauga County      3620  1765   1855       3.0   \n",
       "6  1001020700  Alabama  Autauga County      3420  1459   1961       4.0   \n",
       "7  1001020801  Alabama  Autauga County      2913  1495   1418       4.2   \n",
       "8  1001020802  Alabama  Autauga County     11333  5488   5845       1.4   \n",
       "9  1001020900  Alabama  Autauga County      6167  3111   3056       1.0   \n",
       "\n",
       "   White  Black  Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  \\\n",
       "0   86.3    5.2     0.0  ...   0.5          0.0         2.1         24.5   \n",
       "1   41.6   54.5     0.0  ...   0.0          0.5         0.0         22.2   \n",
       "2   61.4   26.5     0.6  ...   1.0          0.8         1.5         23.1   \n",
       "3   80.3    7.1     0.5  ...   1.5          2.9         2.1         25.9   \n",
       "4   77.5   16.4     0.0  ...   0.8          0.3         0.7         21.0   \n",
       "5   70.7   25.1     0.0  ...   0.7          3.5         8.0         21.1   \n",
       "6   78.0   13.7     0.6  ...   0.0          0.0         0.0         16.4   \n",
       "7   86.3    7.1     1.5  ...   0.0          0.7         5.2         24.6   \n",
       "8   81.8   15.3     0.0  ...   0.0          0.0         2.2         27.4   \n",
       "9   86.2    9.7     0.8  ...   0.0          5.0         5.9         35.6   \n",
       "\n",
       "   Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0       881         74.2        21.2           4.5         0.0           4.6  \n",
       "1       852         75.9        15.0           9.0         0.0           3.4  \n",
       "2      1482         73.3        21.1           4.8         0.7           4.7  \n",
       "3      1849         75.8        19.7           4.5         0.0           6.1  \n",
       "4      4787         71.4        24.1           4.5         0.0           2.3  \n",
       "5      1364         84.0        14.1           1.9         0.0           6.1  \n",
       "6      1418         77.4        17.6           5.0         0.0          16.9  \n",
       "7      1369         72.7        17.5           8.8         1.0           7.2  \n",
       "8      4857         70.1        22.5           7.4         0.0           2.2  \n",
       "9      2781         80.4        14.5           5.0         0.0           6.8  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prxWWbiuP5wn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prxWWbiuP5wn",
    "outputId": "b8fe6b00-3dec-46e4-af02-dc8d887049fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe information is: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74001 entries, 0 to 74000\n",
      "Data columns (total 37 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   TractId           74001 non-null  int64  \n",
      " 1   State             74001 non-null  object \n",
      " 2   County            74001 non-null  object \n",
      " 3   TotalPop          74001 non-null  int64  \n",
      " 4   Men               74001 non-null  int64  \n",
      " 5   Women             74001 non-null  int64  \n",
      " 6   Hispanic          73305 non-null  float64\n",
      " 7   White             73305 non-null  float64\n",
      " 8   Black             73305 non-null  float64\n",
      " 9   Native            73305 non-null  float64\n",
      " 10  Asian             73305 non-null  float64\n",
      " 11  Pacific           73305 non-null  float64\n",
      " 12  VotingAgeCitizen  74001 non-null  int64  \n",
      " 13  Income            72885 non-null  float64\n",
      " 14  IncomeErr         72885 non-null  float64\n",
      " 15  IncomePerCap      73256 non-null  float64\n",
      " 16  IncomePerCapErr   73256 non-null  float64\n",
      " 17  Poverty           73159 non-null  float64\n",
      " 18  ChildPoverty      72891 non-null  float64\n",
      " 19  Professional      73190 non-null  float64\n",
      " 20  Service           73190 non-null  float64\n",
      " 21  Office            73190 non-null  float64\n",
      " 22  Construction      73190 non-null  float64\n",
      " 23  Production        73190 non-null  float64\n",
      " 24  Drive             73200 non-null  float64\n",
      " 25  Carpool           73200 non-null  float64\n",
      " 26  Transit           73200 non-null  float64\n",
      " 27  Walk              73200 non-null  float64\n",
      " 28  OtherTransp       73200 non-null  float64\n",
      " 29  WorkAtHome        73200 non-null  float64\n",
      " 30  MeanCommute       73055 non-null  float64\n",
      " 31  Employed          74001 non-null  int64  \n",
      " 32  PrivateWork       73190 non-null  float64\n",
      " 33  PublicWork        73190 non-null  float64\n",
      " 34  SelfEmployed      73190 non-null  float64\n",
      " 35  FamilyWork        73190 non-null  float64\n",
      " 36  Unemployment      73191 non-null  float64\n",
      "dtypes: float64(29), int64(6), object(2)\n",
      "memory usage: 20.9+ MB\n"
     ]
    }
   ],
   "source": [
    "print('Dataframe information is:','\\n')\n",
    "census.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5t90esQQg4b",
   "metadata": {
    "id": "g5t90esQQg4b"
   },
   "source": [
    "1.2 Remove observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "RK6j5KVHQmrS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RK6j5KVHQmrS",
    "outputId": "86a25dad-6b78-4cc4-d976-6f08638bfe1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncomeErr           1116\n",
       "Income              1116\n",
       "ChildPoverty        1110\n",
       "MeanCommute          946\n",
       "Poverty              842\n",
       "FamilyWork           811\n",
       "SelfEmployed         811\n",
       "PublicWork           811\n",
       "PrivateWork          811\n",
       "Production           811\n",
       "Construction         811\n",
       "Office               811\n",
       "Service              811\n",
       "Professional         811\n",
       "Unemployment         810\n",
       "Transit              801\n",
       "Drive                801\n",
       "WorkAtHome           801\n",
       "OtherTransp          801\n",
       "Walk                 801\n",
       "Carpool              801\n",
       "IncomePerCapErr      745\n",
       "IncomePerCap         745\n",
       "White                696\n",
       "Black                696\n",
       "Native               696\n",
       "Asian                696\n",
       "Pacific              696\n",
       "Hispanic             696\n",
       "State                  0\n",
       "VotingAgeCitizen       0\n",
       "Employed               0\n",
       "Women                  0\n",
       "Men                    0\n",
       "TotalPop               0\n",
       "County                 0\n",
       "TractId                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_missing_value_feature = census.isnull().sum().sort_values(ascending = False)\n",
    "total_missing_value_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "el-zRKVKmA-t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el-zRKVKmA-t",
    "outputId": "2b16981c-ff71-4847-ba0f-ce3e918b1dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 72718 entries, 0 to 74000\n",
      "Data columns (total 37 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   TractId           72718 non-null  int64  \n",
      " 1   State             72718 non-null  object \n",
      " 2   County            72718 non-null  object \n",
      " 3   TotalPop          72718 non-null  int64  \n",
      " 4   Men               72718 non-null  int64  \n",
      " 5   Women             72718 non-null  int64  \n",
      " 6   Hispanic          72718 non-null  float64\n",
      " 7   White             72718 non-null  float64\n",
      " 8   Black             72718 non-null  float64\n",
      " 9   Native            72718 non-null  float64\n",
      " 10  Asian             72718 non-null  float64\n",
      " 11  Pacific           72718 non-null  float64\n",
      " 12  VotingAgeCitizen  72718 non-null  int64  \n",
      " 13  Income            72718 non-null  float64\n",
      " 14  IncomeErr         72718 non-null  float64\n",
      " 15  IncomePerCap      72718 non-null  float64\n",
      " 16  IncomePerCapErr   72718 non-null  float64\n",
      " 17  Poverty           72718 non-null  float64\n",
      " 18  ChildPoverty      72718 non-null  float64\n",
      " 19  Professional      72718 non-null  float64\n",
      " 20  Service           72718 non-null  float64\n",
      " 21  Office            72718 non-null  float64\n",
      " 22  Construction      72718 non-null  float64\n",
      " 23  Production        72718 non-null  float64\n",
      " 24  Drive             72718 non-null  float64\n",
      " 25  Carpool           72718 non-null  float64\n",
      " 26  Transit           72718 non-null  float64\n",
      " 27  Walk              72718 non-null  float64\n",
      " 28  OtherTransp       72718 non-null  float64\n",
      " 29  WorkAtHome        72718 non-null  float64\n",
      " 30  MeanCommute       72718 non-null  float64\n",
      " 31  Employed          72718 non-null  int64  \n",
      " 32  PrivateWork       72718 non-null  float64\n",
      " 33  PublicWork        72718 non-null  float64\n",
      " 34  SelfEmployed      72718 non-null  float64\n",
      " 35  FamilyWork        72718 non-null  float64\n",
      " 36  Unemployment      72718 non-null  float64\n",
      "dtypes: float64(29), int64(6), object(2)\n",
      "memory usage: 21.1+ MB\n"
     ]
    }
   ],
   "source": [
    "census_imputed=census.dropna(axis=0)\n",
    "census_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "UhF0Jn7NQ4dG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhF0Jn7NQ4dG",
    "outputId": "b36ec600-6630-4b34-a4da-e62b67e46e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of duplicates: \" + str(census_imputed.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UIcrX1-7RAc4",
   "metadata": {
    "id": "UIcrX1-7RAc4"
   },
   "source": [
    "1.3 Encodeing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8TQ1WCJZRFL9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "8TQ1WCJZRFL9",
    "outputId": "9c4fb869-0c97-4227-daa3-0609bae56b8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "census_imputed"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c2d93006-ced6-483a-a165-350d06c8dfad\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TractId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001020100</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>1845</td>\n",
       "      <td>899</td>\n",
       "      <td>946</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>881</td>\n",
       "      <td>74.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001020200</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>2172</td>\n",
       "      <td>1167</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>852</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001020300</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>3385</td>\n",
       "      <td>1533</td>\n",
       "      <td>1852</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>1482</td>\n",
       "      <td>73.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001020400</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>4267</td>\n",
       "      <td>2001</td>\n",
       "      <td>2266</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1849</td>\n",
       "      <td>75.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001020500</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>9965</td>\n",
       "      <td>5054</td>\n",
       "      <td>4911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4787</td>\n",
       "      <td>71.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1001020600</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>3620</td>\n",
       "      <td>1765</td>\n",
       "      <td>1855</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.7</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>1364</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001020700</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>3420</td>\n",
       "      <td>1459</td>\n",
       "      <td>1961</td>\n",
       "      <td>4.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>1418</td>\n",
       "      <td>77.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1001020801</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>2913</td>\n",
       "      <td>1495</td>\n",
       "      <td>1418</td>\n",
       "      <td>4.2</td>\n",
       "      <td>86.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>24.6</td>\n",
       "      <td>1369</td>\n",
       "      <td>72.7</td>\n",
       "      <td>17.5</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1001020802</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>11333</td>\n",
       "      <td>5488</td>\n",
       "      <td>5845</td>\n",
       "      <td>1.4</td>\n",
       "      <td>81.8</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>27.4</td>\n",
       "      <td>4857</td>\n",
       "      <td>70.1</td>\n",
       "      <td>22.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1001020900</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>6167</td>\n",
       "      <td>3111</td>\n",
       "      <td>3056</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>35.6</td>\n",
       "      <td>2781</td>\n",
       "      <td>80.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2d93006-ced6-483a-a165-350d06c8dfad')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c2d93006-ced6-483a-a165-350d06c8dfad button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c2d93006-ced6-483a-a165-350d06c8dfad');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-49888cff-aa5d-4268-9795-59c3f02337bb\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49888cff-aa5d-4268-9795-59c3f02337bb')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-49888cff-aa5d-4268-9795-59c3f02337bb button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      TractId  State  County  TotalPop   Men  Women  Hispanic  White  Black  \\\n",
       "0  1001020100      0      89      1845   899    946       2.4   86.3    5.2   \n",
       "1  1001020200      0      89      2172  1167   1005       1.1   41.6   54.5   \n",
       "2  1001020300      0      89      3385  1533   1852       8.0   61.4   26.5   \n",
       "3  1001020400      0      89      4267  2001   2266       9.6   80.3    7.1   \n",
       "4  1001020500      0      89      9965  5054   4911       0.9   77.5   16.4   \n",
       "5  1001020600      0      89      3620  1765   1855       3.0   70.7   25.1   \n",
       "6  1001020700      0      89      3420  1459   1961       4.0   78.0   13.7   \n",
       "7  1001020801      0      89      2913  1495   1418       4.2   86.3    7.1   \n",
       "8  1001020802      0      89     11333  5488   5845       1.4   81.8   15.3   \n",
       "9  1001020900      0      89      6167  3111   3056       1.0   86.2    9.7   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.0  ...   0.5          0.0         2.1         24.5       881   \n",
       "1     0.0  ...   0.0          0.5         0.0         22.2       852   \n",
       "2     0.6  ...   1.0          0.8         1.5         23.1      1482   \n",
       "3     0.5  ...   1.5          2.9         2.1         25.9      1849   \n",
       "4     0.0  ...   0.8          0.3         0.7         21.0      4787   \n",
       "5     0.0  ...   0.7          3.5         8.0         21.1      1364   \n",
       "6     0.6  ...   0.0          0.0         0.0         16.4      1418   \n",
       "7     1.5  ...   0.0          0.7         5.2         24.6      1369   \n",
       "8     0.0  ...   0.0          0.0         2.2         27.4      4857   \n",
       "9     0.8  ...   0.0          5.0         5.9         35.6      2781   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         74.2        21.2           4.5         0.0           4.6  \n",
       "1         75.9        15.0           9.0         0.0           3.4  \n",
       "2         73.3        21.1           4.8         0.7           4.7  \n",
       "3         75.8        19.7           4.5         0.0           6.1  \n",
       "4         71.4        24.1           4.5         0.0           2.3  \n",
       "5         84.0        14.1           1.9         0.0           6.1  \n",
       "6         77.4        17.6           5.0         0.0          16.9  \n",
       "7         72.7        17.5           8.8         1.0           7.2  \n",
       "8         70.1        22.5           7.4         0.0           2.2  \n",
       "9         80.4        14.5           5.0         0.0           6.8  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "states = census_imputed['State']\n",
    "census_imputed['State']  = le.fit_transform(census_imputed['State'])\n",
    "states = census_imputed['County']\n",
    "census_imputed['County'] = le.fit_transform(census_imputed['County'])\n",
    "census_imputed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5LEU9ZncRRk6",
   "metadata": {
    "id": "5LEU9ZncRRk6"
   },
   "source": [
    "1.4 Removing the \"TractId\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "CyapEf1dRTSb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "CyapEf1dRTSb",
    "outputId": "c16d020b-7ee9-4123-d047-50f8f6e7ce30"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "census_imputed"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-045a6644-89f6-4528-9f2a-bd8be15cb00c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>1845</td>\n",
       "      <td>899</td>\n",
       "      <td>946</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>881</td>\n",
       "      <td>74.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>2172</td>\n",
       "      <td>1167</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>852</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>3385</td>\n",
       "      <td>1533</td>\n",
       "      <td>1852</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>1482</td>\n",
       "      <td>73.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>4267</td>\n",
       "      <td>2001</td>\n",
       "      <td>2266</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1849</td>\n",
       "      <td>75.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>9965</td>\n",
       "      <td>5054</td>\n",
       "      <td>4911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4787</td>\n",
       "      <td>71.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-045a6644-89f6-4528-9f2a-bd8be15cb00c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-045a6644-89f6-4528-9f2a-bd8be15cb00c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-045a6644-89f6-4528-9f2a-bd8be15cb00c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-35391d80-3f4d-4b67-9318-bc2ea7bf51b3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35391d80-3f4d-4b67-9318-bc2ea7bf51b3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-35391d80-3f4d-4b67-9318-bc2ea7bf51b3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   State  County  TotalPop   Men  Women  Hispanic  White  Black  Native  \\\n",
       "0      0      89      1845   899    946       2.4   86.3    5.2     0.0   \n",
       "1      0      89      2172  1167   1005       1.1   41.6   54.5     0.0   \n",
       "2      0      89      3385  1533   1852       8.0   61.4   26.5     0.6   \n",
       "3      0      89      4267  2001   2266       9.6   80.3    7.1     0.5   \n",
       "4      0      89      9965  5054   4911       0.9   77.5   16.4     0.0   \n",
       "\n",
       "   Asian  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0    1.2  ...   0.5          0.0         2.1         24.5       881   \n",
       "1    1.0  ...   0.0          0.5         0.0         22.2       852   \n",
       "2    0.7  ...   1.0          0.8         1.5         23.1      1482   \n",
       "3    0.2  ...   1.5          2.9         2.1         25.9      1849   \n",
       "4    3.1  ...   0.8          0.3         0.7         21.0      4787   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         74.2        21.2           4.5         0.0           4.6  \n",
       "1         75.9        15.0           9.0         0.0           3.4  \n",
       "2         73.3        21.1           4.8         0.7           4.7  \n",
       "3         75.8        19.7           4.5         0.0           6.1  \n",
       "4         71.4        24.1           4.5         0.0           2.3  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del census_imputed['TractId']\n",
    "census_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G75dQZYPRb3C",
   "metadata": {
    "id": "G75dQZYPRb3C"
   },
   "source": [
    "The next two requirements will need to be completed together as they might depend on one another:\n",
    "\n",
    "[.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain.\n",
    "[.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is no need to split the data multiple times for this lab.\n",
    "\n",
    "Note: You will need to one hot encode the target, but do not one hot encode the categorical data until instructed to do so in the lab.\n",
    "\n",
    "1.5 Balancing of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cmDukVgvRdp4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmDukVgvRdp4",
    "outputId": "027677e8-3f4a-4fe0-e2c1-d1f609a47e51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers for 1st quartile : 18019\n",
      "The numbers for 2nd quartile : 18220\n",
      "The numbers for 3rd quartile : 18214\n",
      "The numbers for 4th quartile : 18170\n"
     ]
    }
   ],
   "source": [
    "ChildPoverty_value=census_imputed['ChildPoverty']\n",
    "ChildPoverty_value = np.sort(ChildPoverty_value)\n",
    "\n",
    "q1 = np.percentile(ChildPoverty_value, 25)\n",
    "q2 = np.percentile(ChildPoverty_value, 50)\n",
    "q3 = np.percentile(ChildPoverty_value, 75)\n",
    "\n",
    "count1 = np.sum(ChildPoverty_value < q1)\n",
    "count2 = np.sum((ChildPoverty_value >= q1) & (ChildPoverty_value < q2))\n",
    "count3 = np.sum((ChildPoverty_value >= q2) & (ChildPoverty_value < q3))\n",
    "count4 = np.sum(ChildPoverty_value > q3)\n",
    "\n",
    "print(\"The numbers for 1st quartile :\", count1)\n",
    "print(\"The numbers for 2nd quartile :\", count2)\n",
    "print(\"The numbers for 3rd quartile :\", count3)\n",
    "print(\"The numbers for 4th quartile :\", count4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jS6dRW3NRym8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "jS6dRW3NRym8",
    "outputId": "46f6eee5-566b-4683-cc52-49bee86cab0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChildPoverty\n",
      "0    18229\n",
      "1    18171\n",
      "2    18148\n",
      "3    18170\n",
      "Name: ChildPoverty, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def child_poverty_label(x):\n",
    "    if x <= q1:\n",
    "        return 0\n",
    "    if x > q1 and x <= q2:\n",
    "        return 1\n",
    "    if x> q2 and x <= q3:\n",
    "        return 2\n",
    "\n",
    "    return 3\n",
    "\n",
    "census_imputed['ChildPoverty'] = census_imputed['ChildPoverty'].apply(child_poverty_label)\n",
    "census_imputed.groupby(['ChildPoverty'])['ChildPoverty'].count()\n",
    "\n",
    "\n",
    "CPC = census_imputed.groupby(['ChildPoverty'])['ChildPoverty'].count()\n",
    "print(CPC)\n",
    "\n",
    "ticks = np.unique(census_imputed['ChildPoverty'])\n",
    "plt.bar(np.unique(census_imputed['ChildPoverty']), CPC)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=ticks, labels=np.unique(census_imputed['ChildPoverty']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8oBa8iw6R_LP",
   "metadata": {
    "id": "8oBa8iw6R_LP"
   },
   "source": [
    "1.6 Splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wREXyhLiSA9D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wREXyhLiSA9D",
    "outputId": "5dc6d935-9967-4479-cd8d-df7b1d875412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58174, 35)\n",
      "(58174,)\n",
      "(14544, 35)\n",
      "(14544,)\n"
     ]
    }
   ],
   "source": [
    "y = census_imputed['ChildPoverty']\n",
    "X = census_imputed\n",
    "del X['ChildPoverty']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20,stratify=y, random_state=7324,  shuffle=True)\n",
    "\n",
    "# Convert the Data frame to array\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oISefuk4mB7y",
   "metadata": {
    "id": "oISefuk4mB7y"
   },
   "source": [
    "## 2. Pre-processing and Initial Modeling (2.5 points total)\n",
    "\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unlNf_xv-beS",
   "metadata": {
    "id": "unlNf_xv-beS"
   },
   "outputs": [],
   "source": [
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "\n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Testing acc:',accuracy_score(y_test,yhat))\n",
    "\n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Testing Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mtb80qj3mPI5",
   "metadata": {
    "id": "Mtb80qj3mPI5"
   },
   "source": [
    "### 2.1 Two-layer perception:\n",
    "\n",
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mc5MZuAd-f01",
   "metadata": {
    "id": "mc5MZuAd-f01"
   },
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)  # last layer sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2) # back prop the sensitivity\n",
    "\n",
    "        grad2 = V2 @ A2.T # no bias on final layer\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bw9h0J1f_Fiu",
   "metadata": {
    "id": "bw9h0J1f_Fiu"
   },
   "outputs": [],
   "source": [
    "# mini-batching\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        rho_W1_prev = np.zeros(self.W1.shape)\n",
    "        rho_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta / (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2)\n",
    "\n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 = eta * grad1, eta * grad2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev))\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev))\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Jch8cdOS_Qjc",
   "metadata": {
    "id": "Jch8cdOS_Qjc"
   },
   "outputs": [],
   "source": [
    "## Cross Entropy Loss\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mxHuL549_aZ0",
   "metadata": {
    "id": "mxHuL549_aZ0"
   },
   "outputs": [],
   "source": [
    "# proper Glorot initialization\n",
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1))\n",
    "        W2[:,:1] = 0\n",
    "\n",
    "        return W1, W2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cuwIoQzVAZmI",
   "metadata": {
    "id": "cuwIoQzVAZmI"
   },
   "source": [
    "### 2.2 Model 1: neither normalized nor one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z8PzrXVqAh2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8PzrXVqAh2a",
    "outputId": "9cf55615-ec00-47ff-eced-579b4414bbc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 48/60"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':50,\n",
    "         'C':0.01, 'epochs':60, 'eta':0.01,\n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'minibatches':len(X_train)/256, 'shuffle':True, 'random_state':1}\n",
    "\n",
    "nn_cross_without = TLPBetterInitial(**vals)\n",
    "nn_cross_without.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AWe0hmJ7C131",
   "metadata": {
    "id": "AWe0hmJ7C131"
   },
   "outputs": [],
   "source": [
    "cost_avgs = [np.mean(x) for x in nn_cross_without.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Row data training loss curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ISiiKHxCC5mv",
   "metadata": {
    "id": "ISiiKHxCC5mv"
   },
   "outputs": [],
   "source": [
    "print_result(nn_cross_without,X_train,y_train,X_test,y_test,color=\"blue\")\n",
    "plt.title('Row data Testing accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ABwFvd0FC9pN",
   "metadata": {
    "id": "ABwFvd0FC9pN"
   },
   "outputs": [],
   "source": [
    "yhat_without = nn_cross_without.predict(X_test)\n",
    "Acc_without=accuracy_score(y_test,yhat_without)\n",
    "\n",
    "cm = confusion_matrix(y_test,yhat_without)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0jf_yOgImYq0",
   "metadata": {
    "id": "0jf_yOgImYq0"
   },
   "source": [
    "### model 2: normalize the continuous numeric feature data\n",
    "\n",
    "[.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LZmlNEdQDPIh",
   "metadata": {
    "id": "LZmlNEdQDPIh"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Numerical_columns = census.select_dtypes(include=[\"number\"]).columns.tolist();\n",
    "Numerical_columns.remove('ChildPoverty')\n",
    "Numerical_columns.remove('TractId')\n",
    "\n",
    "normalized_values = X[Numerical_columns].values\n",
    "\n",
    "scaler =  StandardScaler()\n",
    "\n",
    "scaler.fit(normalized_values)\n",
    "values_to_normalized = scaler.transform(normalized_values)\n",
    "\n",
    "X_normalized =  copy.deepcopy(X)\n",
    "X_normalized[Numerical_columns] = values_to_normalized\n",
    "\n",
    "X_normalized\n",
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "akatn_DTDVfU",
   "metadata": {
    "id": "akatn_DTDVfU"
   },
   "outputs": [],
   "source": [
    "X_N_train, X_N_test, y_N_train, y_N_test = train_test_split(X_normalized, y,test_size=0.20,stratify=y, random_state=7324,  shuffle=True)\n",
    "X_N_train = X_N_train.to_numpy()\n",
    "X_N_test = X_N_test.to_numpy()\n",
    "y_N_train = y_N_train.to_numpy()\n",
    "y_N_test = y_N_test.to_numpy()\n",
    "\n",
    "vals = {'n_hidden':50,\n",
    "         'C':1e-2, 'epochs':40, 'eta':0.01,  'random_state':1,\n",
    "         'alpha':0.1, 'decrease_const':0.1, 'minibatches':len(X_train)/256,\n",
    "         'shuffle':True}\n",
    "\n",
    "\n",
    "nn_cross_normalized = TLPBetterInitial(**vals)\n",
    "nn_cross_normalized.fit(X_N_train, y_N_train, print_progress=1, XY_test=(X_N_test, y_N_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgHi2phCEZbR",
   "metadata": {
    "id": "fgHi2phCEZbR"
   },
   "outputs": [],
   "source": [
    "cost_avgs = [np.mean(x) for x in nn_cross_normalized.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Normalized data training loss curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qRbNZ-ysEdEQ",
   "metadata": {
    "id": "qRbNZ-ysEdEQ"
   },
   "outputs": [],
   "source": [
    "print_result(nn_cross_normalized,X_N_train,y_N_train,X_N_test,y_N_test,color=\"blue\")\n",
    "plt.title('Normalized data Testing accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170XwSjsEhP0",
   "metadata": {
    "id": "170XwSjsEhP0"
   },
   "outputs": [],
   "source": [
    "yhat_normalized = nn_cross_normalized.predict(X_N_test)\n",
    "Acc_normalized=accuracy_score(y_N_test,yhat_normalized)\n",
    "\n",
    "cm = confusion_matrix(y_N_test,yhat_normalized)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etQapH4wmdNp",
   "metadata": {
    "id": "etQapH4wmdNp"
   },
   "source": [
    "### 2.4 [.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data.\n",
    "\n",
    "Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wudr5zBxErIE",
   "metadata": {
    "id": "Wudr5zBxErIE"
   },
   "outputs": [],
   "source": [
    "categorical_columns = census.select_dtypes(include=[\"object\"]).columns.tolist();\n",
    "X_encode =  copy.deepcopy(X_normalized)\n",
    "\n",
    "tmp_df = pd.get_dummies(X_encode.State,prefix='State')\n",
    "X_encode = pd.concat((X_encode,tmp_df),axis=1) # add back into the dataframe\n",
    "tmp_df = pd.get_dummies(X_encode.State,prefix='County')\n",
    "X_encode = pd.concat((X_encode,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "\n",
    "del X_encode['State']\n",
    "del X_encode['County']\n",
    "\n",
    "X_encode.head()\n",
    "\n",
    "X_f_train, X_f_test, y_f_train, y_f_test = train_test_split(X_encode, y,test_size=0.20,stratify=y, random_state=7324,  shuffle=True)\n",
    "X_f_train = X_f_train.to_numpy()\n",
    "X_f_test = X_f_test.to_numpy()\n",
    "y_f_train = y_f_train.to_numpy()\n",
    "y_f_test = y_f_test.to_numpy()\n",
    "\n",
    "X_f_train=X_f_train.astype(float)\n",
    "X_f_test=X_f_test.astype(float)\n",
    "y_f_train=y_f_train.astype(float)\n",
    "y_f_test=y_f_test.astype(float)\n",
    "\n",
    "vals = {'n_hidden':50,\n",
    "         'C':1e-2, 'epochs':135, 'eta':0.01,\n",
    "         'alpha':0.01, 'decrease_const':0.1, 'minibatches':len(X_train)/256,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_cross_encode = TLPBetterInitial(**vals)\n",
    "nn_cross_encode.fit(X_f_train, y_f_train, print_progress=1, XY_test=(X_f_test, y_f_test));\n",
    "\n",
    "cost_avgs = [np.mean(x) for x in nn_cross_encode.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Normalized-Encoded data training loss curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emwlfsAEFUvd",
   "metadata": {
    "id": "emwlfsAEFUvd"
   },
   "outputs": [],
   "source": [
    "print_result(nn_cross_encode,X_f_train,y_f_train,X_f_test,y_f_test,color=\"blue\")\n",
    "plt.title('Normalized-Encoded Testing accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KAtFLtxOFZJr",
   "metadata": {
    "id": "KAtFLtxOFZJr"
   },
   "outputs": [],
   "source": [
    "yhat_encode = nn_cross_encode.predict(X_f_test)\n",
    "Acc_encode=accuracy_score(y_f_test,yhat_encode)\n",
    "cm = confusion_matrix(y_f_test,yhat_encode)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xWGo6Os0mgk4",
   "metadata": {
    "id": "xWGo6Os0mgk4"
   },
   "source": [
    "### 2.5 [1 points] Compare the performance of the three models you just trained.\n",
    "\n",
    "Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DyqdgsODmJmG",
   "metadata": {
    "id": "DyqdgsODmJmG"
   },
   "outputs": [],
   "source": [
    "t = 2.26 / np.sqrt(10)\n",
    "\n",
    "e12 = (1-Acc_without)-(1-Acc_normalized)\n",
    "e13 = (1-Acc_without)-(1-Acc_encode)\n",
    "e23 = (1-Acc_normalized)-(1-Acc_encode)\n",
    "\n",
    "stdtot12 = np.std(e12)\n",
    "stdtot13 = np.std(e13)\n",
    "stdtot23 = np.std(e23)\n",
    "\n",
    "dbar12 = np.mean(e12)\n",
    "dbar13 = np.mean(e13)\n",
    "dbar23 = np.mean(e23)\n",
    "\n",
    "\n",
    "print(f'1 vs. 2: The error range is from {dbar12 - t*stdtot12} to {dbar12+ t*stdtot12}')\n",
    "print(f'1 vs. 3: The error range is from {dbar13 - t*stdtot13} to {dbar13+ t*stdtot13}')\n",
    "print(f'2 vs. 3: The error range is from {dbar23 - t*stdtot23} to {dbar23+ t*stdtot23}')\n",
    "\n",
    "print(''' From the result, we can't determine which model is better.\\n Therefore, we will use the McNemar test for comparison as shown in the next section.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4lJGupOOF_Gj",
   "metadata": {
    "id": "4lJGupOOF_Gj"
   },
   "outputs": [],
   "source": [
    "print('The comparison between Model1 and Model2 with 95% confidence interval is as follows:')\n",
    "table1 = mcnemar_table(y_target=y_test, y_model1=yhat_without, y_model2=yhat_normalized)\n",
    "chi12_, p12 = mcnemar(ary=table1, corrected=True)\n",
    "print(table1)\n",
    "print(f\"chi² statistic: {chi12_}, p-value: {p12}\")\n",
    "if p12 > 0.05:\n",
    "    print(\"Same proportions of errors (failed to reject Null Hypothesis--There isn't enough evidence to suggest that Models 1 and 2 are different from each other )\")\n",
    "else:\n",
    "        print('Different proportions of errors (reject Null Hypothesis)')\n",
    "\n",
    "\n",
    "print('The comparison between Model1 and Model3 with 95% confidence interval is as follows:')\n",
    "table2 = mcnemar_table(y_target=y_test, y_model1=yhat_without, y_model2=yhat_encode)\n",
    "chi13_, p13 = mcnemar(ary=table2, corrected=True)\n",
    "print(table2)\n",
    "print(f\"chi² statistic: {chi13_}, p-value: {p13}\")\n",
    "if p13 > 0.05:\n",
    "    print('Same proportions of errors (fail to reject Null Hypothesis)')\n",
    "else:\n",
    "        print('Different proportions of errors (able to reject Null Hypothesis---so based on the accuracy, Model3 is better than Model1)')\n",
    "\n",
    "\n",
    "print('The comparison between Model2 and Model3 with 95% confidence interval is as follows:')\n",
    "table3 = mcnemar_table(y_target=y_test, y_model1=yhat_normalized, y_model2=yhat_encode)\n",
    "chi23_, p23 = mcnemar(ary=table3, corrected=True)\n",
    "print(table3)\n",
    "print(f\"chi² statistic: {chi23_}, p-value: {p23}\")\n",
    "if p13 > 0.05:\n",
    "    print('Same proportions of errors (fail to reject Null Hypothesis)')\n",
    "else:\n",
    "        print('Different proportions of errors (able to reject Null Hypothesis, so based on the accuracy Model3 is better than Model2)')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Based on the results above, the peformacne of Model3 is beter than Models 1 and 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mSm4jTm6mjaS",
   "metadata": {
    "id": "mSm4jTm6mjaS"
   },
   "source": [
    "## Modeling (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4CR4PiXmnlv",
   "metadata": {
    "id": "d4CR4PiXmnlv"
   },
   "source": [
    "[1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mGV1a61nGWyU",
   "metadata": {
    "id": "mGV1a61nGWyU"
   },
   "outputs": [],
   "source": [
    "class ThreeLayerPerceptronBase(object):\n",
    "    def __init__(self, n1_hidden=30,n2_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n1_hidden = n1_hidden\n",
    "        self.n2_hidden = n2_hidden   # Number of hidden in third layer.\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n1_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n1_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "        W2_num_elems = (self.n1_hidden + 1)*self.n2_hidden_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n2_hidden_, self.n1_hidden + 1)\n",
    "\n",
    "        W3_num_elems = (self.n2_hidden + 1)*self.n_output_\n",
    "        W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "        W3 = W3.reshape(self.n_output_, self.n2_hidden + 1)\n",
    "        return W1, W2 , W3\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A4)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2,W3)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2,W3):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        A3 = self._add_bias_unit(A3, how='row')\n",
    "        Z3 = W3 @ A3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V3 = -2*(Y_enc-A4)*A4*(1-A4)  # last layer sensitivity\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3) # back prop the sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2[1:,:]) # back prop the sensitivity\n",
    "\n",
    "\n",
    "        grad3 = V3 @ A3.T # no bias on final layer\n",
    "        grad2 = V2[1:,:] @ A2.T # dont back prop sensitivity of bias\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "        return grad1, grad2, grad3\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _,_, _, A4 = self._feedforward(X, self.W1, self.W2,self.W3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fPHw_oHDGhIM",
   "metadata": {
    "id": "fPHw_oHDGhIM"
   },
   "outputs": [],
   "source": [
    "# mini-batching\n",
    "class THLPMiniBatch(ThreeLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 , self.W3 = self._initialize_weights()\n",
    "\n",
    "        rho_W1_prev = np.zeros(self.W1.shape)\n",
    "        rho_W2_prev = np.zeros(self.W2.shape)\n",
    "        rho_W3_prev = np.zeros(self.W3.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "\n",
    "\n",
    "        # You will update These arrays, initialized here\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta / (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            mini_grad1 = []\n",
    "            mini_grad2 = []\n",
    "            mini_grad3 = []\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2,A3, Z3, A4 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.W3)\n",
    "\n",
    "                cost = self._cost(A4,Y_enc[:, idx],self.W1,self.W2,self.W3)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 ,grad3 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, Z1=Z1, Z2=Z2, Z3=Z3,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2, W3=self.W3)\n",
    "\n",
    "\n",
    "                mini_grad1.append(grad1)\n",
    "                mini_grad2.append(grad2)\n",
    "                mini_grad3.append(grad3)\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 , rho_W3 = eta * grad1, eta * grad2,eta * grad3\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev))\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev))\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev))\n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev = rho_W1, rho_W2 , rho_W3\n",
    "            # # keep track of the\n",
    "            # #    average magnitude of gradient of each layer\n",
    "            # #    grad_w1_ and grad_w2_\n",
    "            # #    and make the eta values for each adaptive\n",
    "\n",
    "            # a = np.mean(np.abs(grad1))\n",
    "            # b = np.mean(np.abs(grad2))\n",
    "            # c = np.mean(np.abs(grad3))\n",
    "\n",
    "            # grad1 *= 2*c/(a+b+c) #mixin parameters\n",
    "            # grad2 *= 2*a/(a+b+c)\n",
    "            # grad3 *= 2*b/(a+b+c)\n",
    "\n",
    "            # # Track the magnitude of the gradient\n",
    "            # self.grad_w1_[i] = np.mean(np.abs(np.mean(mini_grad1[:])))\n",
    "            # self.grad_w2_[i] = np.mean(np.abs(np.mean(mini_grad2[:])))\n",
    "            # self.grad_w3_[i] = np.mean(np.abs(np.mean(mini_grad3[:])))\n",
    "\n",
    "            # #Update weights and biases\n",
    "            # self.W1 -= self.eta * grad1\n",
    "            # self.W2 -= self.eta * grad2\n",
    "            # self.W3 -= self.eta * grad3\n",
    "\n",
    "            # # Track the magnitude of the gradient update here\n",
    "            # # This should be AFTER applying your dynamic scaling\n",
    "            # # That is, you SHOULD include eta here.\n",
    "            # self.update_w1_[i] = np.mean(np.abs(grad1))\n",
    "            # self.update_w2_[i] = np.mean(np.abs(grad2))\n",
    "            # self.update_w3_[i] = np.mean(np.abs(grad3))\n",
    "            # Track the magnitude of the gradient\n",
    "            self.grad_w1_[i] = np.mean(np.abs(np.mean(mini_grad1[:])))\n",
    "            self.grad_w2_[i] = np.mean(np.abs(np.mean(mini_grad2[:])))\n",
    "            self.grad_w3_[i] = np.mean(np.abs(np.mean(mini_grad3[:])))\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JI-Pp8mkGzrL",
   "metadata": {
    "id": "JI-Pp8mkGzrL"
   },
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "class THLPMiniBatchCrossEntropy(THLPMiniBatch):\n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A4)+(1-Y_enc)*np.log(1-A4))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2,W3)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2 ,W3):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V3 = (A4-Y_enc) # <- this is only line that changed\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2[1:,:])\n",
    "\n",
    "        grad3 = V3 @ A3.T\n",
    "        grad2 = V2[1:,:] @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2 , grad3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dwOsbprlG6cw",
   "metadata": {
    "id": "dwOsbprlG6cw"
   },
   "outputs": [],
   "source": [
    "# proper Glorot initialization\n",
    "class THLPBetterInitial(THLPMiniBatchCrossEntropy):\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n1_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n1_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n2_hidden + self.n1_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n2_hidden, self.n1_hidden + 1))\n",
    "        W2[:,:1] = 0\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n2_hidden + 1))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n2_hidden + 1))\n",
    "        W3[:,:1] = 0\n",
    "\n",
    "        return W1, W2,W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_VqXqEydHD3F",
   "metadata": {
    "id": "_VqXqEydHD3F"
   },
   "outputs": [],
   "source": [
    "vals = {'n1_hidden':70, 'n2_hidden':50,\n",
    "         'C':1e-2, 'epochs':120, 'eta':0.01, 'random_state':1,\n",
    "         'alpha':0.1, 'decrease_const':0.1, 'minibatches':len(X_train)/256,\n",
    "         'shuffle':True}\n",
    "\n",
    "nn_cross_three = THLPBetterInitial(**vals)\n",
    "nn_cross_three.fit(X_f_train, y_f_train, print_progress=1, XY_test=(X_f_test, y_f_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xFxuPTjVIKwl",
   "metadata": {
    "id": "xFxuPTjVIKwl"
   },
   "outputs": [],
   "source": [
    "print_result(nn_cross_three,X_f_train,y_f_train,X_f_test,y_f_test,color=\"blue\")\n",
    "plt.title('Testing accuracy for three layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T4lYkgT5IOH0",
   "metadata": {
    "id": "T4lYkgT5IOH0"
   },
   "outputs": [],
   "source": [
    "cost_avgs = [np.mean(x) for x in nn_cross_three.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training loss curve of three layer')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uKsXPMS4ISBR",
   "metadata": {
    "id": "uKsXPMS4ISBR"
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_cross_three.grad_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(nn_cross_three.grad_w2_[10:]), label='w2')\n",
    "plt.plot(np.abs(nn_cross_three.grad_w3_[10:]), label='w3')\n",
    "plt.legend()\n",
    "plt.title('Three layer')\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NX8yNrQbIVX7",
   "metadata": {
    "id": "NX8yNrQbIVX7"
   },
   "outputs": [],
   "source": [
    "yhat_three = nn_cross_three.predict(X_f_test)\n",
    "Acc_three=accuracy_score(y_f_test,yhat_three)\n",
    "cm = confusion_matrix(y_f_test,yhat_three)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pQa2IICLmvsj",
   "metadata": {
    "id": "pQa2IICLmvsj"
   },
   "source": [
    "[1 points] Repeat the previous step, adding support for a fourth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5pcItb-xmmRL",
   "metadata": {
    "id": "5pcItb-xmmRL"
   },
   "outputs": [],
   "source": [
    "class FourLayerPerceptronBase(object):\n",
    "    def __init__(self, n1_hidden=30,n2_hidden=30,n3_hidden=10,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n1_hidden = n1_hidden\n",
    "        self.n2_hidden = n2_hidden   # Number of hidden in third layer.\n",
    "        self.n3_hidden = n3_hidden   # Number of hidden in fourth layer.\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n1_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n1_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "        W2_num_elems = (self.n1_hidden + 1)*self.n2_hidden_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n2_hidden_, self.n1_hidden + 1)\n",
    "\n",
    "        W3_num_elems = (self.n2_hidden + 1)*self.n3_hidden_\n",
    "        W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "        W3 = W3.reshape(self.n3_hidden_, self.n2_hidden + 1)\n",
    "\n",
    "\n",
    "        W4_num_elems = (self.n3_hidden + 1)*self.n_output_\n",
    "        W4 = np.random.uniform(-1.0, 1.0, size=W4_num_elems)\n",
    "        W4 = W4.reshape(self.n_output_, self.n3_hidden + 1)\n",
    "        return W1, W2 , W3 , W4\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3, W4):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2)++ np.mean(W4[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A5,Y_enc,W1, W2, W3, W4):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A5)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2, W3, W4):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        A3 = self._add_bias_unit(A3, how='row')\n",
    "        Z3 = W3 @ A3\n",
    "\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        A4 = self._add_bias_unit(A4, how='row')\n",
    "        Z4 = W4 @ A4\n",
    "\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, Z1, Z2, Z3, Z4, Y_enc, W1, W2, W3, W4):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V4 = -2*(Y_enc-A5)*A5*(1-A5)  # last layer sensitivity\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4) # back prop the sensitivity\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3[1:,:]) # back prop the sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2[1:,:]) # back prop the sensitivity\n",
    "\n",
    "\n",
    "        grad4 = V4 @ A4.T # no bias on final layer\n",
    "        grad3 = V3[1:,:] @ A3.T # dont back prop sensitivity of bias\n",
    "        grad2 = V2[1:,:] @ A2.T # dont back prop sensitivity of bias\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "        grad4[:, 1:] += W4[:, 1:] * self.l2_C\n",
    "        return grad1, grad2, grad3, grad4\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _,_, _,_, _,  A5 = self._feedforward(X, self.W1, self.W2, self.W3, self.W4)\n",
    "        y_pred = np.argmax(A5, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "# mini-batching\n",
    "class FLPMiniBatch(FourLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 , self.W3 ,self.W4 = self._initialize_weights()\n",
    "\n",
    "        rho_W1_prev = np.zeros(self.W1.shape)\n",
    "        rho_W2_prev = np.zeros(self.W2.shape)\n",
    "        rho_W3_prev = np.zeros(self.W3.shape)\n",
    "        rho_W4_prev = np.zeros(self.W4.shape)\n",
    "\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "\n",
    "\n",
    "        # You will update These arrays, initialized here\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta / (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            mini_grad1 = []\n",
    "            mini_grad2 = []\n",
    "            mini_grad3 = []\n",
    "            mini_grad4 = []\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2,A3, Z3,A4, Z4, A5 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.W3,\n",
    "                                                       self.W4)\n",
    "\n",
    "                cost = self._cost(A5,Y_enc[:, idx],self.W1,self.W2,self.W3,self.W4)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 ,grad3,grad4 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, A5=A5, Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1, W2=self.W2, W3=self.W3, W4=self.W4)\n",
    "\n",
    "\n",
    "                mini_grad1.append(grad1)\n",
    "                mini_grad2.append(grad2)\n",
    "                mini_grad3.append(grad3)\n",
    "                mini_grad4.append(grad4)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 , rho_W3 , rho_W4 = eta * grad1, eta * grad2,eta * grad3 ,eta * grad4\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev))\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev))\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev))\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev))\n",
    "\n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev = rho_W1, rho_W2 , rho_W3, rho_W4\n",
    "\n",
    "                        # Track the magnitude of the gradient\n",
    "            self.grad_w1_[i] = np.mean(np.abs(np.mean(mini_grad1[:])))\n",
    "            self.grad_w2_[i] = np.mean(np.abs(np.mean(mini_grad2[:])))\n",
    "            self.grad_w3_[i] = np.mean(np.abs(np.mean(mini_grad3[:])))\n",
    "            self.grad_w4_[i] = np.mean(np.abs(np.mean(mini_grad4[:])))\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "# Cross Entropy Loss\n",
    "\n",
    "class FLPMiniBatchCrossEntropy(FLPMiniBatch):\n",
    "    def _cost(self,A5,Y_enc,W1,W2,W3,W4):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A5)+(1-Y_enc)*np.log(1-A5))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, Z1, Z2, Z3, Z4, Y_enc, W1, W2 ,W3, W4):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V4 = (A5-Y_enc) # <- this is only line that changed\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3[1:,:])\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2[1:,:])\n",
    "\n",
    "        grad4 = V4 @ A4.T\n",
    "        grad3 = V3[1:,:] @ A3.T\n",
    "        grad2 = V2[1:,:] @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "        grad4[:, 1:] += W4[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2 , grad3 , grad4\n",
    "\n",
    "\n",
    "\n",
    "# proper Glorot initialization\n",
    "\n",
    "class FLPBetterInitial(FLPMiniBatchCrossEntropy):\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n1_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n1_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n2_hidden + self.n1_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n2_hidden, self.n1_hidden + 1))\n",
    "        W2[:,:1] = 0\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n3_hidden + self.n2_hidden + 1))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n3_hidden, self.n2_hidden + 1))\n",
    "        W3[:,:1] = 0\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n3_hidden + 1))\n",
    "        W4 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n3_hidden + 1))\n",
    "        W4[:,:1] = 0\n",
    "\n",
    "        return W1, W2, W3, W4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_R3TlPzy__j6",
   "metadata": {
    "id": "_R3TlPzy__j6"
   },
   "outputs": [],
   "source": [
    "vals2 = {'n1_hidden':70, 'n2_hidden':50, 'n3_hidden':30,\n",
    "         'C':1e-2, 'epochs':140, 'eta':0.01, 'random_state':1,\n",
    "         'alpha':0.1, 'decrease_const':0.1, 'minibatches':len(X_train)/256,\n",
    "         'shuffle':True}\n",
    "\n",
    "nn_cross_four = FLPBetterInitial(**vals2)\n",
    "nn_cross_four.fit(X_f_train, y_f_train, print_progress=1, XY_test=(X_f_test, y_f_test));\n",
    "\n",
    "print_result(nn_cross_four,X_f_train,y_f_train,X_f_test,y_f_test,color=\"blue\")\n",
    "plt.title('Testing accuracy for four layer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UGKdCWUjAR4T",
   "metadata": {
    "id": "UGKdCWUjAR4T"
   },
   "outputs": [],
   "source": [
    "cost_avgs = [np.mean(x) for x in nn_cross_four.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training loss curve for four layer perceptrons')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cn0tfOotAU7Y",
   "metadata": {
    "id": "Cn0tfOotAU7Y"
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_cross_four.grad_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(nn_cross_four.grad_w2_[10:]), label='w2')\n",
    "plt.plot(np.abs(nn_cross_four.grad_w3_[10:]), label='w3')\n",
    "plt.plot(np.abs(nn_cross_four.grad_w4_[10:]), label='w4')\n",
    "plt.legend()\n",
    "plt.title('Four layer perceptrons')\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GzklungLAXtp",
   "metadata": {
    "id": "GzklungLAXtp"
   },
   "outputs": [],
   "source": [
    "yhat_four = nn_cross_four.predict(X_f_test)\n",
    "Acc_four=accuracy_score(y_f_test,yhat_four)\n",
    "cm = confusion_matrix(y_f_test,yhat_four)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oX83Nn0Imxpp",
   "metadata": {
    "id": "oX83Nn0Imxpp"
   },
   "source": [
    "[1 points] Repeat the previous step, adding support for a fifth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YN9-AJWYmziS",
   "metadata": {
    "id": "YN9-AJWYmziS"
   },
   "outputs": [],
   "source": [
    "class FiveLayerPerceptronBase(object):\n",
    "    def __init__(self, n1_hidden=30,n2_hidden=30,n3_hidden=10,n4_hidden=8,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n1_hidden = n1_hidden\n",
    "        self.n2_hidden = n2_hidden   # Number of hidden in third layer.\n",
    "        self.n3_hidden = n3_hidden   # Number of hidden in fourth layer.\n",
    "        self.n4_hidden = n4_hidden   # Number of hidden in fourth layer.\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n1_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n1_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "        W2_num_elems = (self.n1_hidden + 1)*self.n2_hidden_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n2_hidden_, self.n1_hidden + 1)\n",
    "\n",
    "        W3_num_elems = (self.n2_hidden + 1)*self.n3_hidden_\n",
    "        W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "        W3 = W3.reshape(self.n3_hidden_, self.n2_hidden + 1)\n",
    "\n",
    "        W4_num_elems = (self.n3_hidden + 1)*self.n4_hidden_\n",
    "        W4 = np.random.uniform(-1.0, 1.0, size=W4_num_elems)\n",
    "        W4 = W3.reshape(self.n4_hidden_, self.n3_hidden + 1)\n",
    "\n",
    "        W5_num_elems = (self.n4_hidden + 1)*self.n_output_\n",
    "        W5 = np.random.uniform(-1.0, 1.0, size=W5_num_elems)\n",
    "        W5 = W4.reshape(self.n_output_, self.n4_hidden + 1)\n",
    "        return W1, W2 , W3 , W4 , W5\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3, W4,W5):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2)\n",
    "                                       + np.mean(W4[:, 1:] ** 2) + np.mean(W5[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A6,Y_enc,W1, W2, W3, W4, W5):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A6)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2, W3, W4, W5):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        A3 = self._add_bias_unit(A3, how='row')\n",
    "        Z3 = W3 @ A3\n",
    "\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        A4 = self._add_bias_unit(A4, how='row')\n",
    "        Z4 = W4 @ A4\n",
    "\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        A5 = self._add_bias_unit(A5, how='row')\n",
    "        Z5 = W5 @ A5\n",
    "\n",
    "        A6 = self._sigmoid(Z5)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5,A6, Z1, Z2, Z3, Z4, Z5,  Y_enc, W1, W2, W3, W4, W5):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V5 = -2*(Y_enc-A6)*A6*(1-A6)  # last layer sensitivity\n",
    "        V4 = A5*(1-A5)*(W5.T @ V5) # back prop the sensitivity\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4[1:,:]) # back prop the sensitivity\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3[1:,:]) # back prop the sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2[1:,:]) # back prop the sensitivity\n",
    "\n",
    "\n",
    "        grad5 = V5 @ A5.T # no bias on final layer\n",
    "        grad4 = V4[1:,:] @ A4.T # dont back prop sensitivity of bias\n",
    "        grad3 = V3[1:,:] @ A3.T # dont back prop sensitivity of bias\n",
    "        grad2 = V2[1:,:] @ A2.T # dont back prop sensitivity of bias\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "        grad4[:, 1:] += W4[:, 1:] * self.l2_C\n",
    "        grad5[:, 1:] += W5[:, 1:] * self.l2_C\n",
    "        return grad1, grad2, grad3, grad4 , grad5\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _,_, _,_, _, _, _, A6 = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.W5)\n",
    "        y_pred = np.argmax(A6, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "# mini-batching\n",
    "class FiLPMiniBatch(FiveLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0,\n",
    "                 decrease_iter = 10,shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 , self.W3 ,self.W4 ,self.W5 = self._initialize_weights()\n",
    "\n",
    "        rho_W1_prev = np.zeros(self.W1.shape)\n",
    "        rho_W2_prev = np.zeros(self.W2.shape)\n",
    "        rho_W3_prev = np.zeros(self.W3.shape)\n",
    "        rho_W4_prev = np.zeros(self.W4.shape)\n",
    "        rho_W5_prev = np.zeros(self.W5.shape)\n",
    "\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "\n",
    "\n",
    "        # You will update These arrays, initialized here\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.grad_w5_ = np.zeros(self.epochs)\n",
    "\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            mini_grad1 = []\n",
    "            mini_grad2 = []\n",
    "            mini_grad3 = []\n",
    "            mini_grad4 = []\n",
    "            mini_grad5 = []\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2,A3, Z3,A4, Z4, A5, Z5, A6 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.W3,\n",
    "                                                       self.W4,\n",
    "                                                       self.W5)\n",
    "\n",
    "                cost = self._cost(A6,Y_enc[:, idx],self.W1,self.W2,self.W3,self.W4,self.W5)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 ,grad3,grad4 ,grad5 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, A5=A5,A6 =A6,\n",
    "                                                  Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4, Z5=Z5,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1, W2=self.W2, W3=self.W3, W4=self.W4, W5=self.W5)\n",
    "\n",
    "\n",
    "                mini_grad1.append(grad1)\n",
    "                mini_grad2.append(grad2)\n",
    "                mini_grad3.append(grad3)\n",
    "                mini_grad4.append(grad4)\n",
    "                mini_grad5.append(grad5)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 , rho_W3 , rho_W4 ,rho_W5 = eta * grad1, eta * grad2,eta * grad3 ,eta * grad4 ,eta * grad5\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev))\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev))\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev))\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev))\n",
    "                self.W5 -= (rho_W5 + (self.alpha * rho_W5_prev))\n",
    "\n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev ,rho_W5_prev = rho_W1, rho_W2 , rho_W3, rho_W4 , rho_W5\n",
    "\n",
    "                        # Track the magnitude of the gradient\n",
    "            self.grad_w1_[i] = np.mean(np.abs(np.mean(mini_grad1[:])))\n",
    "            self.grad_w2_[i] = np.mean(np.abs(np.mean(mini_grad2[:])))\n",
    "            self.grad_w3_[i] = np.mean(np.abs(np.mean(mini_grad3[:])))\n",
    "            self.grad_w4_[i] = np.mean(np.abs(np.mean(mini_grad4[:])))\n",
    "            self.grad_w5_[i] = np.mean(np.abs(np.mean(mini_grad5[:])))\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "# Cross Entropy Loss\n",
    "\n",
    "class FiLPMiniBatchCrossEntropy(FiLPMiniBatch):\n",
    "    def _cost(self,A6,Y_enc,W1,W2,W3,W4,W5):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A6)+(1-Y_enc)*np.log(1-A6))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, A6, Z1, Z2, Z3, Z4, Z5, Y_enc, W1, W2 ,W3, W4, W5):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V5 = (A6-Y_enc) # <- this is only line that changed\n",
    "        V4 = A5*(1-A5)*(W5.T @ V5)\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4[1:,:])\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3[1:,:])\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2[1:,:])\n",
    "\n",
    "        grad5 = V5 @ A5.T\n",
    "        grad4 = V4[1:,:] @ A4.T\n",
    "        grad3 = V3[1:,:] @ A3.T\n",
    "        grad2 = V2[1:,:] @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "        grad4[:, 1:] += W4[:, 1:] * self.l2_C\n",
    "        grad5[:, 1:] += W5[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2 , grad3 , grad4 ,grad5\n",
    "\n",
    "\n",
    "\n",
    "# proper Glorot initialization\n",
    "\n",
    "class FiLPBetterInitial(FiLPMiniBatchCrossEntropy):\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n1_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n1_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n2_hidden + self.n1_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n2_hidden, self.n1_hidden + 1))\n",
    "        W2[:,:1] = 0\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n3_hidden + self.n2_hidden + 1))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n3_hidden, self.n2_hidden + 1))\n",
    "        W3[:,:1] = 0\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n4_hidden + self.n3_hidden + 1))\n",
    "        W4 = np.random.uniform(-init_bound, init_bound,(self.n4_hidden, self.n3_hidden + 1))\n",
    "        W4[:,:1] = 0\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n4_hidden + 1))\n",
    "        W5 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n4_hidden + 1))\n",
    "        W5[:,:1] = 0\n",
    "\n",
    "        return W1, W2, W3, W4 ,W5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iVPso8cBAzO_",
   "metadata": {
    "id": "iVPso8cBAzO_"
   },
   "outputs": [],
   "source": [
    "vals5 = {'n1_hidden':70, 'n2_hidden':50, 'n3_hidden':30,  'n4_hidden':10,\n",
    "         'C':1e-2, 'epochs':120, 'eta':0.01, 'random_state':1,\n",
    "         'alpha':0.1, 'decrease_const':0.1, 'minibatches':len(X_train)/256,\n",
    "         'shuffle':True}\n",
    "\n",
    "nn_cross_five = FiLPBetterInitial(**vals5)\n",
    "nn_cross_five.fit(X_f_train, y_f_train, print_progress=1, XY_test=(X_f_test, y_f_test));\n",
    "\n",
    "print_result(nn_cross_five,X_f_train,y_f_train,X_f_test,y_f_test,color=\"blue\")\n",
    "plt.title('Testing accuracy for five layer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9uz3WBOuDaPZ",
   "metadata": {
    "id": "9uz3WBOuDaPZ"
   },
   "outputs": [],
   "source": [
    "cost_avgs = [np.mean(x) for x in nn_cross_five.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training loss curve for five layer perceptrons')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JSLp2dazDd1I",
   "metadata": {
    "id": "JSLp2dazDd1I"
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_cross_five.grad_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(nn_cross_five.grad_w2_[10:]), label='w2')\n",
    "plt.plot(np.abs(nn_cross_five.grad_w3_[10:]), label='w3')\n",
    "plt.plot(np.abs(nn_cross_five.grad_w4_[10:]), label='w4')\n",
    "plt.plot(np.abs(nn_cross_five.grad_w5_[10:]), label='w5')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.title('Five layer perceptrons')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iWhqcCPnDhO2",
   "metadata": {
    "id": "iWhqcCPnDhO2"
   },
   "outputs": [],
   "source": [
    "yhat_five = nn_cross_five.predict(X_f_test)\n",
    "Acc_five=accuracy_score(y_f_test,yhat_five)\n",
    "cm = confusion_matrix(y_f_test,yhat_five)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q0-QZOYxm0bp",
   "metadata": {
    "id": "q0-QZOYxm0bp"
   },
   "source": [
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (choose either RMSProp or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eVlnKlB-m2QX",
   "metadata": {
    "id": "eVlnKlB-m2QX"
   },
   "outputs": [],
   "source": [
    "class FiLPRMSProp(FiLPBetterInitial):\n",
    "\n",
    "    def __init__(self, eps=0.0, gamma=0.0, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) ##\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5 = self._initialize_weights()\n",
    "\n",
    "                 # keep track of gradients of each epoch\n",
    "        V1_prev = np.zeros(self.W1.shape)\n",
    "        V2_prev = np.zeros(self.W2.shape)\n",
    "        V3_prev = np.zeros(self.W3.shape)\n",
    "        V4_prev = np.zeros(self.W4.shape)\n",
    "        V5_prev = np.zeros(self.W5.shape)\n",
    "\n",
    "                # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # for momentum\n",
    "        rho_W4_prev = np.zeros(self.W4.shape) # for momentum\n",
    "        rho_W5_prev = np.zeros(self.W5.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.grad_w5_ = np.zeros(self.epochs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            mini_grad1 = []\n",
    "            mini_grad2 = []\n",
    "            mini_grad3 = []\n",
    "            mini_grad4 = []\n",
    "            mini_grad5 = []\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5,Z5, A6 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2, self.W3, self.W4,self.W5)\n",
    "\n",
    "                cost = self._cost(A6, Y_enc[:, idx],self.W1,self.W2, self.W3,self.W4, self.W5 )\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradW4, gradW5  = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, A5=A5,\n",
    "                                                  A6=A6,\n",
    "                                                  Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4,Z5=Z5,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1, W2=self.W2, W3=self.W3,\n",
    "                                                  W4=self.W4, W5=self.W5)\n",
    "\n",
    "\n",
    "                V1 = self.gamma*V1_prev + (1-self.gamma)*gradW1*gradW1\n",
    "                V2 = self.gamma*V2_prev + (1-self.gamma)*gradW2*gradW2\n",
    "                V3 = self.gamma*V3_prev + (1-self.gamma)*gradW3*gradW3\n",
    "                V4 = self.gamma*V4_prev + (1-self.gamma)*gradW4*gradW4\n",
    "                V5 = self.gamma*V5_prev + (1-self.gamma)*gradW5*gradW5\n",
    "\n",
    "\n",
    "                # momentum and adagrad\n",
    "                rho_W1, rho_W2,rho_W3,rho_W4,rho_W5 = eta * gradW1/np.sqrt(V1 +self.eps), eta * gradW2/np.sqrt(V2 +self.eps),eta * gradW3/np.sqrt(V3+self.eps),eta * gradW4/np.sqrt(V4+self.eps),eta * gradW5/np.sqrt(V5+self.eps)\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # update with momentum\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev)) # update with momentum\n",
    "                self.W5 -= (rho_W5 + (self.alpha * rho_W5_prev)) # update with momentum\n",
    "\n",
    "\n",
    "                # update previous parameters for the\n",
    "                rho_W1_prev, rho_W2_prev,rho_W3_prev,rho_W4_prev,rho_W5_prev = rho_W1, rho_W2, rho_W3, rho_W4, rho_W5\n",
    "                V1_prev, V2_prev, V3_prev, V4_prev, V5_prev = V1, V2, V3,V4,V5\n",
    "\n",
    "\n",
    "                 # keep track of gradients of each minibach in each epoch\n",
    "                mini_grad1.append(gradW1)\n",
    "                mini_grad2.append(gradW2)\n",
    "                mini_grad3.append(gradW3)\n",
    "                mini_grad4.append(gradW4)\n",
    "                mini_grad5.append(gradW5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            self.grad_w1_[i] = np.mean(np.abs((np.mean(mini_grad1[:]))))\n",
    "            self.grad_w2_[i] = np.mean(np.abs((np.mean(mini_grad2[:]))))\n",
    "            self.grad_w3_[i] = np.mean(np.abs((np.mean(mini_grad3[:]))))\n",
    "            self.grad_w4_[i] = np.mean(np.abs((np.mean(mini_grad4[:]))))\n",
    "            self.grad_w5_[i] = np.mean(np.abs((np.mean(mini_grad5[:]))))\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RSAmLkAxJoYC",
   "metadata": {
    "id": "RSAmLkAxJoYC"
   },
   "outputs": [],
   "source": [
    "vals_Ap = {'n1_hidden':70, 'n2_hidden':50, 'n3_hidden':30,  'n4_hidden':10,\n",
    "         'C':1e-2, 'epochs':120, 'eta':0.01, 'random_state':1, 'decrease_iter':20, 'eps':1e-3, 'gamma':0.9,\n",
    "         'alpha':0.1, 'decrease_const':0.1, 'minibatches':len(X_train)/256,\n",
    "         'shuffle':True}\n",
    "\n",
    "nn_Ap = FiLPRMSProp(**vals_Ap)\n",
    "nn_Ap.fit(X_f_train, y_f_train, print_progress=1, XY_test=(X_f_test, y_f_test));\n",
    "\n",
    "print_result(nn_Ap,X_f_train,y_f_train,X_f_test,y_f_test,color=\"blue\")\n",
    "plt.title('Testing accuracy for five layer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IzfZBZrBKRHz",
   "metadata": {
    "id": "IzfZBZrBKRHz"
   },
   "outputs": [],
   "source": [
    "cost_avgs = [np.mean(x) for x in nn_Ap.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training loss curve for five layer perceptrons with RMSProp')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mW4HYesOKTjZ",
   "metadata": {
    "id": "mW4HYesOKTjZ"
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_Ap.grad_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(nn_Ap.grad_w2_[10:]), label='w2')\n",
    "plt.plot(np.abs(nn_Ap.grad_w3_[10:]), label='w3')\n",
    "plt.plot(np.abs(nn_Ap.grad_w4_[10:]), label='w4')\n",
    "plt.plot(np.abs(nn_Ap.grad_w5_[10:]), label='w5')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.title('Five layer perceptrons with RMSProp')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lmsIiZe3KWLn",
   "metadata": {
    "id": "lmsIiZe3KWLn"
   },
   "outputs": [],
   "source": [
    "yhat_five_RMSProp = nn_Ap.predict(X_f_test)\n",
    "Acc_five_RMSProp=accuracy_score(y_f_test,yhat_five_RMSProp)\n",
    "cm = confusion_matrix(y_f_test,yhat_five_RMSProp)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_D2l-qCHK8Dc",
   "metadata": {
    "id": "_D2l-qCHK8Dc"
   },
   "outputs": [],
   "source": [
    "# Comparison:\n",
    "\n",
    "print('The comparison between FLP \"without RMSProp\" and \"with RMSProp\" with 95% confidence interval is as follows:')\n",
    "table = mcnemar_table(y_target=y_test, y_model1=yhat_five, y_model2=yhat_five_RMSProp)\n",
    "chi2_, p = mcnemar(ary=table1, corrected=True)\n",
    "print(table)\n",
    "print(f\"chi² statistic: {chi2_}, p-value: {p}\")\n",
    "if p > 0.05:\n",
    "    print(\"Same proportions of errors (fail to reject Null Hypothesis--There isn't enough evidence to suggest that the Five Layer Perceptron models with and without RMSProp are different from each other)\")\n",
    "else:\n",
    "        print('Different proportions of errors (reject H0)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'                   With RMSProp              Without RMSProp')\n",
    "\n",
    "print(f' Accuracy        {Acc_five_RMSProp}          {Acc_five}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dKb1IB3vXFVX",
   "metadata": {
    "id": "dKb1IB3vXFVX"
   },
   "source": [
    "## Exceptional Work (1 points total)\n",
    "\n",
    "### Adaptive Momentum (Adam) Application in the Five-Layer Perceptron.\n",
    "\n",
    "Adam is a deep neural network training technique that uses adaptive learning rate optimization. It combines the benefits of AdaGrad and RMSProp, two more stochastic gradient descent additions. Adam uses momentum to enhance convergence and calculates adaptive learning rates for every parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lfplqD8GXLmH",
   "metadata": {
    "id": "lfplqD8GXLmH"
   },
   "outputs": [],
   "source": [
    "class FiLPAdam(FiLPBetterInitial):\n",
    "    def __init__(self, eps=1e-7, beta1=0.9, beta2=0.999, **kwds):\n",
    "        # Initialize AdaM parameters\n",
    "        self.eps = eps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        # Initialize with other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        # Track gradients of each epoch\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.grad_w5_ = np.zeros(self.epochs)\n",
    "\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # Initialize weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # Get initial accuracy\n",
    "        self.score_.append(accuracy_score(y_data, self.predict(X_data)))\n",
    "        # Track validation if provided\n",
    "        if XY_test is not None:\n",
    "            X_test, y_test = XY_test\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test, self.predict(X_test)))\n",
    "\n",
    "        M1_prev = np.zeros_like(self.W1)\n",
    "        M2_prev = np.zeros_like(self.W2)\n",
    "        M3_prev = np.zeros_like(self.W3)\n",
    "        M4_prev = np.zeros_like(self.W4)\n",
    "        M5_prev = np.zeros_like(self.W5)\n",
    "\n",
    "        V1_prev = np.zeros_like(self.W1)\n",
    "        V2_prev = np.zeros_like(self.W2)\n",
    "        V3_prev = np.zeros_like(self.W3)\n",
    "        V4_prev = np.zeros_like(self.W4)\n",
    "        V5_prev = np.zeros_like(self.W5)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if print_progress > 0 and (epoch + 1) % print_progress == 0:\n",
    "                sys.stderr.write(f'\\rEpoch: {epoch + 1}/{self.epochs}')\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini_batches = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            mini_grad1 = []\n",
    "            mini_grad2 = []\n",
    "            mini_grad3 = []\n",
    "            mini_grad4 = []\n",
    "            mini_grad5 = []\n",
    "\n",
    "            for idx in mini_batches:\n",
    "                # Feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6 = self._feedforward(X_data[idx],\n",
    "                                                                               self.W1,\n",
    "                                                                               self.W2, self.W3, self.W4, self.W5)\n",
    "\n",
    "                cost = self._cost(A6, Y_enc[:, idx], self.W1, self.W2, self.W3, self.W4, self.W5)\n",
    "                mini_cost.append(cost)  # Append cost of mini-batch\n",
    "\n",
    "                # Compute gradient via backpropagation\n",
    "                G1, G2, G3, G4, G5 = self._get_gradient(A1, A2, A3, A4, A5, A6,\n",
    "                                                        Z1, Z2, Z3, Z4, Z5,\n",
    "                                                        Y_enc[:, idx],\n",
    "                                                        self.W1, self.W2, self.W3, self.W4, self.W5)\n",
    "\n",
    "                M1 = self.beta1 * M1_prev + (1 - self.beta1) * G1\n",
    "                M2 = self.beta1 * M2_prev + (1 - self.beta1) * G2\n",
    "                M3 = self.beta1 * M3_prev + (1 - self.beta1) * G3\n",
    "                M4 = self.beta1 * M4_prev + (1 - self.beta1) * G4\n",
    "                M5 = self.beta1 * M5_prev + (1 - self.beta1) * G5\n",
    "\n",
    "                V1 = self.beta2 * V1_prev + (1 - self.beta2) * (G1 ** 2)\n",
    "                V2 = self.beta2 * V2_prev + (1 - self.beta2) * (G2 ** 2)\n",
    "                V3 = self.beta2 * V3_prev + (1 - self.beta2) * (G3 ** 2)\n",
    "                V4 = self.beta2 * V4_prev + (1 - self.beta2) * (G4 ** 2)\n",
    "                V5 = self.beta2 * V5_prev + (1 - self.beta2) * (G5 ** 2)\n",
    "\n",
    "                M1_hat = M1 / (1 - self.beta1 ** (epoch + 1))\n",
    "                M2_hat = M2 / (1 - self.beta1 ** (epoch + 1))\n",
    "                M3_hat = M3 / (1 - self.beta1 ** (epoch + 1))\n",
    "                M4_hat = M4 / (1 - self.beta1 ** (epoch + 1))\n",
    "                M5_hat = M5 / (1 - self.beta1 ** (epoch + 1))\n",
    "\n",
    "                V1_hat = V1 / (1 - self.beta2 ** (epoch + 1))\n",
    "                V2_hat = V2 / (1 - self.beta2 ** (epoch + 1))\n",
    "                V3_hat = V3 / (1 - self.beta2 ** (epoch + 1))\n",
    "                V4_hat = V4 / (1 - self.beta2 ** (epoch + 1))\n",
    "                V5_hat = V5 / (1 - self.beta2 ** (epoch + 1))\n",
    "\n",
    "                self.W1 -= self.eta * (M1_hat / (np.sqrt(V1_hat) + self.eps))\n",
    "                self.W2 -= self.eta * (M2_hat / (np.sqrt(V2_hat) + self.eps))\n",
    "                self.W3 -= self.eta * (M3_hat / (np.sqrt(V3_hat) + self.eps))\n",
    "                self.W4 -= self.eta * (M4_hat / (np.sqrt(V4_hat) + self.eps))\n",
    "                self.W5 -= self.eta * (M5_hat / (np.sqrt(V5_hat) + self.eps))\n",
    "\n",
    "                M1_prev, M2_prev, M3_prev, M4_prev, M5_prev = M1, M2, M3, M4, M5\n",
    "                V1_prev, V2_prev, V3_prev, V4_prev, V5_prev = V1, V2, V3, V4, V5\n",
    "\n",
    "                mini_grad1.append(G1)\n",
    "                mini_grad2.append(G2)\n",
    "                mini_grad3.append(G3)\n",
    "                mini_grad4.append(G4)\n",
    "                mini_grad5.append(G5)\n",
    "\n",
    "            self.grad_w1_[epoch] = np.mean(np.abs(np.mean(mini_grad1)))\n",
    "            self.grad_w2_[epoch] = np.mean(np.abs(np.mean(mini_grad2)))\n",
    "            self.grad_w3_[epoch] = np.mean(np.abs(np.mean(mini_grad3)))\n",
    "            self.grad_w4_[epoch] = np.mean(np.abs(np.mean(mini_grad4)))\n",
    "            self.grad_w5_[epoch] = np.mean(np.abs(np.mean(mini_grad5)))\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data, self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test, self.predict(X_test)))\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bB6lDPqdX50",
   "metadata": {
    "id": "6bB6lDPqdX50"
   },
   "outputs": [],
   "source": [
    "vals_Adam = {'n1_hidden':70, 'n2_hidden':50, 'n3_hidden':30,  'n4_hidden':10, 'beta1':0.999,'beta2':0.9999,\n",
    "         'C':1e-2, 'epochs':250, 'eta':0.01, 'random_state':3000, 'decrease_iter':20,'eps':1e-7,\n",
    "         'alpha':0.1, 'decrease_const':0.1, 'minibatches':len(X_train)/256,\n",
    "         'shuffle':True}\n",
    "\n",
    "nn_Adam = FiLPAdam(**vals_Adam)\n",
    "nn_Adam.fit(X_f_train, y_f_train, print_progress=1, XY_test=(X_f_test, y_f_test));\n",
    "\n",
    "print_result(nn_Adam,X_f_train,y_f_train,X_f_test,y_f_test,color=\"blue\")\n",
    "plt.title('Testing accuracy for five layer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fVKHkXIIedvx",
   "metadata": {
    "id": "fVKHkXIIedvx"
   },
   "outputs": [],
   "source": [
    "cost_avgs = [np.mean(x) for x in nn_Adam.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='blue')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training loss curve for five layer perceptrons with Adam')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vn9qW6Ebe5H0",
   "metadata": {
    "id": "Vn9qW6Ebe5H0"
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(nn_Adam.grad_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(nn_Adam.grad_w2_[10:]), label='w2')\n",
    "plt.plot(np.abs(nn_Adam.grad_w3_[10:]), label='w3')\n",
    "plt.plot(np.abs(nn_Adam.grad_w4_[10:]), label='w4')\n",
    "plt.plot(np.abs(nn_Adam.grad_w5_[10:]), label='w5')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Five layer perceptrons with RMSProp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TlGNU70nfFlP",
   "metadata": {
    "id": "TlGNU70nfFlP"
   },
   "outputs": [],
   "source": [
    "yhat_five_Adam = nn_Adam.predict(X_f_test)\n",
    "Acc_five_Adam =accuracy_score(y_f_test,yhat_five_Adam)\n",
    "cm = confusion_matrix(y_f_test,yhat_five_Adam)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z9gwOluifd-O",
   "metadata": {
    "id": "z9gwOluifd-O"
   },
   "outputs": [],
   "source": [
    "# Statistical Comparison between RMSProp and AdaM Adaptive Learning Techniques\n",
    "\n",
    "print('The comparison between FLP \"RMSProp\" and \"Adam\" with 95% confidence interval is as follows:')\n",
    "table = mcnemar_table(y_target=y_test, y_model1=yhat_five_RMSProp, y_model2=yhat_five_Adam)\n",
    "chi2_, p = mcnemar(ary=table, corrected=True)\n",
    "print(table)\n",
    "print(f\"chi² statistic: {chi2_}, p-value: {p}\")\n",
    "if p > 0.05:\n",
    "    print(\"Same proportions of errors (failed to reject Null Hypothesis--There isn't enough evidence to suggest that the Five Layer Perceptron models with RMSProp and AdaM Adaptive Learning Techniques are different from each other)\")\n",
    "else:\n",
    "    print(\"Different proportions of errors (reject H0)\")\n",
    "\n",
    "print('----------------------------------------------------------------')\n",
    "print(f'                   With RMSProp             With AdaM')\n",
    "print('----------------------------------------------------------------')\n",
    "print(f' Accuracy        {Acc_five_RMSProp:.4f}          {Acc_five_Adam:.4f}')\n",
    "print('----------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x8Rm0J-tw5nc",
   "metadata": {
    "id": "x8Rm0J-tw5nc"
   },
   "source": [
    "We can see from the data that, on average, the AdaM optimizer performs the best\n",
    "\n",
    "---\n",
    "\n",
    "in terms of accuracy and consistent loss function convergence. This reveals how good AdaM is at training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HN8Oyhb-xAik",
   "metadata": {
    "id": "HN8Oyhb-xAik"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
